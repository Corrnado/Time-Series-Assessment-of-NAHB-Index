---
title: "Time Series Assessment of NAHB Index"
output:
  html_document:
    df_print: paged
---

All data was downloaded from website "https://www.nahb.org/en/research/housing-economics/housing-indexes/housing-market-index.aspx". The naitonal NAHB index history data from Jan 2000 to Feb 2018 was converted to csv file in Excel.

```{r}
# import history data from the file and convert the data into time series object
nhist <- read.csv("national_history.csv", header = TRUE)
nhistts <- ts(nhist[,2], frequency = 12, start = c(2000,1), end = c(2018,2))

# get a sense of what the data looks like
plot(nhistts)
hist(nhistts)
summary(nhistts)
nhisttscomp <- decompose(nhistts)
plot(nhisttscomp)
```

NAHB varies from 9 to 74 in the past and most of the time the index falled into 10-20 and 50-70 interval, which should be related to the 2008 finanacial crisis. After decomposign the data, it is shown that there should be a seasonal pattern in the data and there is a rapid drop from 2008. 

```{r}
# to select a time series model using cross validation due to the continuous requirement 
# of time series model, a rolling basis cross validation will be implented

# load necessary library
library(forecast)

# to create a group identifier for each year andthe 2 monthes in 2018 will be combined with 2017
id <- rep(1:18, each = 12)
id <- c(id, c(18, 18))

# to create one numeric vector for storing mae value and one list for ARIMA parameters
models <- vector("list", 17)
maes <- vector("numeric", 17)

# loop through every traning group
for (i in 2:17){
  nhisttsforecast <- auto.arima(window(nhistts, 2000, c(1999 + i, 12)), D = 1, trace = TRUE)
  models[[i]] <- nhisttsforecast$arma
  if (i < 17){
    pred <- forecast(nhisttsforecast, 12)
  } else {
    pred <- forecast(nhisttsforecast, 14)
  }
  maes[i] <- mean(abs(pred$mean - nhistts[id == i + 1]))
}

# using mae criterion for the best fitting model
models[[which.min(maes[2:14]) + 1]]
```

Since time series model requires a continuous training set, random sample from the original group would be a bad idea. Thus, a rolling basis cross validation was introduced that data from 2000 would be training set for fold 1 and the model would be tested on 2001; data from 2000 and 2001 would be training set for fold 2 and the model would be tested on 2002; then the training set includes one more year at the next fold each time and the resulting model would be tested on the following year. Since the original data is from Jan 2000 to Feb 2018, 17 folds were used.

```{r}
# fit the optimal model to the whole data set
nhisttsforecast <- arima(nhistts, order = c(1, 2, 1), seasonal = list(order = c(0, 0, 1), period = 12))
prediction <- forecast(nhisttsforecast, 10)
prediction
plot(prediction)
```

Fit the resulting model to the whole data set and plot the prediciton.
Considering the drop in the index from 2008 to 2012, the index during that period probabaly didn't behave normally comparing to other time. It's worth a try that exclude part of the data from that period into the training data set to yield a model with less abnormal data. Data from Jan 2009 to Dec 2011 was decided to be excluded from the model.

```{r}
# create a new data set with out the drop period
nhist.adj <- rbind(nhist[1:108,],nhist[145:218,])
nhistts.adj <- ts(nhist.adj[,2], frequency = 12, start = 1)


```